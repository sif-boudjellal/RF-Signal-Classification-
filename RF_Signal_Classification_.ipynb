{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca2a84fc",
      "metadata": {
        "scrolled": true,
        "id": "ca2a84fc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os \n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input, UpSampling2D, Dropout ,LSTM, merge\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam # - Works\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f7d1bf3",
      "metadata": {
        "id": "9f7d1bf3"
      },
      "source": [
        "## Data Preprocessing\n",
        "### I use RADIOML 2016.10A Dataset\n",
        "### https://www.deepsig.ai/datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1ZftUU2pR-i",
        "outputId": "f1d0bb18-7145-4b59-bef4-2cd10ad0697c"
      },
      "id": "o1ZftUU2pR-i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_data= '/content/drive/MyDrive/data.pkl'"
      ],
      "metadata": {
        "id": "x7cEk255pXYz"
      },
      "id": "x7cEk255pXYz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "063830d1",
      "metadata": {
        "id": "063830d1"
      },
      "outputs": [],
      "source": [
        "def digitizer(labels):\n",
        "    \n",
        "    unique_labels = np.unique(labels)\n",
        "    label_dict = {}\n",
        "    num = 1\n",
        "    for i in unique_labels:\n",
        "        label_dict[i] = num\n",
        "        num += 1 \n",
        "    \n",
        "    digit_label = []\n",
        "    for i in labels:\n",
        "        digit_label.append(label_dict[i])\n",
        "    \n",
        "    return label_dict,  digit_label \n",
        "             \n",
        "def onehot_encoder(L_dict,  labels):\n",
        "    num_classes = len(L_dict)\n",
        "    vector = np.zeros(num_classes)\n",
        "    vector[L_dict[labels]-1] = 1\n",
        "    return vector\n",
        "\n",
        "with open(path_data, \"rb\") as p:\n",
        "    d = pickle.load(p, encoding='latin1')\n",
        "classes = []    \n",
        "for i in d.keys():    \n",
        "    if i[0] not in classes:\n",
        "        classes.append(i[0])\n",
        "# creating class dictionary for strings to digits transformation.\n",
        "label_dict,  digit_label = digitizer(classes)\n",
        "\n",
        "SNRs = {}\n",
        "for key in d.keys():\n",
        "    if key not in SNRs:\n",
        "        SNRs[key[1]] = []\n",
        "SNRs.keys()\n",
        "\n",
        "j = 0\n",
        "for keys in d.keys():\n",
        "    for arrays in d[keys]:\n",
        "        # convert labels to one-hot encoders.\n",
        "        SNRs[keys[1]].append([onehot_encoder(label_dict, keys[0]),np.array(arrays)]) \n",
        "\n",
        "outfile = open('dataset','wb')\n",
        "pickle.dump(SNRs,outfile)\n",
        "outfile.close()\n",
        "\n",
        "\n",
        "outfile = open('class_dict','wb')\n",
        "pickle.dump(label_dict,outfile)\n",
        "outfile.close()\n",
        "\n",
        "if not os.path.exists('./history'):\n",
        "    os.makedirs('./history')\n",
        "if not os.path.exists('./figures'):\n",
        "    os.makedirs('./figures')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c84e7f54",
      "metadata": {
        "id": "c84e7f54"
      },
      "source": [
        "## Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9d8c8db",
      "metadata": {
        "id": "a9d8c8db"
      },
      "outputs": [],
      "source": [
        "\n",
        "def Cnn_Model():\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same', input_shape=(2,128,1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), padding='valid',  data_format=None))\n",
        "    model.add(Dropout(.3))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), padding='valid', data_format=None))\n",
        "    model.add(Dropout(.3))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), padding='valid', data_format=None))\n",
        "    model.add(Dropout(.3))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(1, 2), padding='valid', data_format=None))\n",
        "    model.add(Dropout(.3))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(11, activation='softmax', kernel_initializer='he_normal'))\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9dd9f4",
      "metadata": {
        "id": "ea9dd9f4"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open('./dataset', 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "   \n",
        "c = 0\n",
        "accuracies_All = []\n",
        "for key in data.keys():\n",
        "    \n",
        "    dataset = []\n",
        "    labels = []\n",
        "    \n",
        "    for values in data[key]:\n",
        "        labels.append(values[0])\n",
        "        dataset.append(values[1]) \n",
        "\n",
        "        \n",
        "    print('Starting training for SNR:', key)\n",
        "    # data spliting\n",
        "\n",
        "    N = len(dataset)\n",
        "    shuffled_indeces = np.random.permutation(range(N))\n",
        "    new_dataset = np.array(dataset)[shuffled_indeces,:,:]\n",
        "    new_labels = np.array(labels)[shuffled_indeces,:]\n",
        "    \n",
        "    num_train = int(0.8*N)\n",
        "    \n",
        "    x_train = new_dataset[:num_train,:,:]\n",
        "    y_train = new_labels[:num_train,:]\n",
        "\n",
        "    num_val = int(0.1*len(x_train))\n",
        "    \n",
        "    x_val = x_train[:num_val,:,:]\n",
        "    x_val = x_val.reshape(x_val.shape[0],x_val.shape[1],x_val.shape[2], -1)\n",
        "    y_val = y_train[:num_val,:]\n",
        "    \n",
        "    x_train = x_train[num_val:,:,:]\n",
        "    x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2], -1)\n",
        "    y_train = y_train[num_val:,:]\n",
        "    \n",
        "    x_test = new_dataset[num_train:,:,:]\n",
        "    x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2], -1)\n",
        "    y_test = new_labels[num_train:,:] \n",
        "\n",
        "    \n",
        "    models = Cnn_Model()\n",
        "    \n",
        " \n",
        "    opt = Adam(learning_rate=0.0001)\n",
        "    models.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    \n",
        "    num_epochs = 50\n",
        "    #earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "    mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_accuracy')\n",
        "    #reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')\n",
        "    #EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
        "    \n",
        "    history = models.fit(x_train,\n",
        "                        y_train,\n",
        "                        epochs=num_epochs,\n",
        "                        batch_size=10,\n",
        "                        callbacks = [mcp_save],\n",
        "                        validation_data=(x_val, y_val))\n",
        "    \n",
        "    \n",
        "    models.load_weights(\".mdl_wts.hdf5\") \n",
        "    loss, acc = models.evaluate(x_test,y_test, verbose=2)\n",
        "    accuracies_All.append([acc,key])   \n",
        "    \n",
        "    prediction = models.predict(x_test)\n",
        "    labels_pred =np.argmax(prediction, axis = 1)\n",
        "    \n",
        "    \n",
        "    with open('./history/SNR_{}_history.pkl'.format(key), 'wb') as file_pi:\n",
        "        pickle.dump(history.history, file_pi)\n",
        "    \n",
        "    DICTED = {'1':labels_pred,'2':y_test}\n",
        "    with open('./history/SNR_{}_prediction.pkl'.format(key), 'wb') as file_pi:\n",
        "        pickle.dump(DICTED, file_pi)\n",
        "        \n",
        "    with open('./SNR_accuracies.pkl', 'wb') as file_pi:\n",
        "        pickle.dump(accuracies_All, file_pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec285d6",
      "metadata": {
        "id": "eec285d6"
      },
      "source": [
        "## Ploting Accuracy and Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf71a0d",
      "metadata": {
        "id": "caf71a0d"
      },
      "outputs": [],
      "source": [
        "for item in os.listdir('history'):\n",
        "    i= item.split('_')\n",
        "    if i[2] == 'history.pkl':\n",
        "        with open('./history/'+item, 'rb') as file:\n",
        "            history = pickle.load(file, encoding = 'latin')\n",
        "        plt.plot(history['loss'], label = 'Train Loss')\n",
        "        plt.plot(history['val_loss'], label = 'Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(f'Model Loss vs. Epoch (SNR= {i[1]})')\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"./figures/SNR {i[1]} -loss.jpg\")\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(history['accuracy'], label = 'Train Accuracy')\n",
        "        plt.plot(history['val_accuracy'], label = 'Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title(f'Model Accuracy vs. Epoch (SNR= {i[1]})')\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"./figures/SNR {i[1]} -Accuracy.jpg\")\n",
        "\n",
        "        plt.show()  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "342288cb",
      "metadata": {
        "id": "342288cb"
      },
      "source": [
        "## Confusion Matrix Comparing Predicted and Target Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82bdb83f",
      "metadata": {
        "id": "82bdb83f"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open(path_data, \"rb\") as p:\n",
        "    d = pickle.load(p, encoding='latin1')\n",
        "\n",
        "classes = []    \n",
        "for i in d.keys():    \n",
        "    if i[0] not in classes:\n",
        "        classes.append(i[0])\n",
        "\n",
        "# creating class dictionary for strings to digits transformation.\n",
        "label_dict,  digit_label = digitizer(classes)\n",
        "\n",
        "for item in os.listdir('history'):\n",
        "    itm= item.split('_')\n",
        "    if itm[2] == 'prediction.pkl':\n",
        "        with open('./history/'+item, 'rb') as file:\n",
        "            y = pickle.load(file, encoding = 'latin')\n",
        "        y_pred = y['1']\n",
        "        y_true = y['2']\n",
        "        y_true = np.argmax(y_true, axis =1)\n",
        "\n",
        "        labels = []\n",
        "        for i in label_dict.items():\n",
        "            labels.append(i[0])\n",
        "\n",
        "        cm = confusion_matrix(y_true,y_pred)\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_figheight(10)\n",
        "        fig.set_figwidth(10)\n",
        "        plt.xticks(ticks=[-1,0,1,2,3,4,5,6,7,8,9,10], rotation=45)\n",
        "        plt.yticks(ticks=[-1,0,1,2,3,4,5,6,7,8,9,10], rotation=45)\n",
        "        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "        ax.figure.colorbar(im, ax=ax)\n",
        "        ax.set_xticklabels([''] + labels)\n",
        "        ax.set_yticklabels([''] + labels)\n",
        "        ax.set (title= f'Confusion Matrix Comparing Predicted and Target Label for SNR {itm[1]}', \n",
        "                ylabel='True label',\n",
        "                xlabel='Predicted label')\n",
        "        fmt = '.2f' \n",
        "        thresh = cm.max() / 2.\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                ax.text(j, i, format(cm[i, j], fmt),\n",
        "                        ha=\"center\", va=\"center\",\n",
        "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        plt.savefig(f\"./figures/SNR {itm[1]} -cm.jpg\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f71c8a8",
      "metadata": {
        "id": "4f71c8a8"
      },
      "source": [
        "## Performance of all Trained Models over RadioML2016.10a Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "349451af",
      "metadata": {
        "id": "349451af"
      },
      "outputs": [],
      "source": [
        "def sorter(dataset1, legend1 = None, dataset2 = None, legend2 = None, dataset3 = None, legend3 = None, model = ''):\n",
        "    \n",
        "    if type(dataset3) == list:\n",
        "        \n",
        "        yyy = []\n",
        "        xxx = []\n",
        "\n",
        "        for i in range(len(dataset3)):\n",
        "            xxx.append(dataset3[i][1])\n",
        "        xxx = sorted(xxx)\n",
        "\n",
        "\n",
        "        for i in xxx:\n",
        "            for j in range(len(dataset3)):\n",
        "                if dataset3[j][1]  == i:\n",
        "                    yyy.append(dataset3[j][0])\n",
        "         \n",
        "    if type(dataset2) == list:\n",
        "        \n",
        "        yy = []\n",
        "        xx = []\n",
        "\n",
        "        for i in range(len(dataset2)):\n",
        "            xx.append(dataset2[i][1])\n",
        "        xx = sorted(xx)\n",
        "\n",
        "\n",
        "        for i in xx:\n",
        "            for j in range(len(dataset2)):\n",
        "                if dataset2[j][1]  == i:\n",
        "                    yy.append(dataset2[j][0])\n",
        " \n",
        "    y = []\n",
        "    x = []\n",
        "\n",
        "    for i in range(len(dataset1)):\n",
        "        x.append(dataset1[i][1])\n",
        "    x = sorted(x)\n",
        "\n",
        "    for i in x:\n",
        "        for j in range(len(dataset1)):\n",
        "            if dataset1[j][1]  == i:\n",
        "                y.append(dataset1[j][0])\n",
        "    \n",
        "    fig = plt.figure(figsize = (10,8))\n",
        "    plt.plot(x,y, linestyle = '--', marker = 'o', label = legend1)\n",
        "    if type(dataset2) == list:\n",
        "            plt.plot(xx,yy, linestyle = '--', marker = 'o', label = legend2)\n",
        "            plt.legend()\n",
        "            \n",
        "    if type(dataset3) == list:\n",
        "            plt.plot(xxx,yyy, linestyle = '--', marker = 'o', label = legend3)\n",
        "            plt.legend()\n",
        "    \n",
        "    plt.title(model)\n",
        "    plt.xlabel('SNR [dB]')\n",
        "    plt.ylabel('Accuracy [%]')\n",
        "    plt.grid()\n",
        "    plt.savefig(f\"./figures/Performance.jpg\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f7eeaa",
      "metadata": {
        "id": "52f7eeaa"
      },
      "outputs": [],
      "source": [
        "with open('./SNR_accuracies.pkl', 'rb') as file:\n",
        "    CNN = pickle.load(file, encoding = 'Latin')\n",
        "sorter(CNN, 'CNN', model = 'Performance of all Trained Models over RadioML2016.10a Dataset')"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "ac6d2790e833bac7006630f8c856bee1f1d4e5343da7c143b1a07d6561db9be1"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "RF Signal Classification .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}